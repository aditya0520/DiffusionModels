11/07/2024 22:44:33 - INFO - __main__ - ***** Training arguments *****
11/07/2024 22:44:33 - INFO - __main__ - Namespace(config='configs/ddpm.yaml', data_dir='./data/imagenet100_128x128/train', image_size=128, batch_size=4, num_workers=8, num_classes=100, run_name='exp-0', output_dir='experiments', num_epochs=10, learning_rate=0.0001, weight_decay=0.0001, grad_clip=1.0, seed=42, mixed_precision='none', num_train_timesteps=1000, num_inference_steps=200, beta_start=0.0002, beta_end=0.02, beta_schedule='linear', variance_type='fixed_small', prediction_type='epsilon', clip_sample=True, clip_sample_range=1.0, unet_in_size=128, unet_in_ch=3, unet_ch=128, unet_ch_mult=[1, 2, 2, 2], unet_attn=[1, 2, 3], unet_num_res_blocks=2, unet_dropout=0.0, latent_ddpm=False, use_cfg=False, cfg_guidance_scale=2.0, use_ddim=False, ckpt='./checkpoint', distributed=False, world_size=1, rank=0, local_rank=0, device='cuda', total_batch_size=4, max_train_steps=216210)
11/07/2024 22:44:33 - INFO - __main__ - ***** Running training *****
11/07/2024 22:44:33 - INFO - __main__ -   Num examples = 86484
11/07/2024 22:44:33 - INFO - __main__ -   Num Epochs = 10
11/07/2024 22:44:33 - INFO - __main__ -   Instantaneous batch size per device = 4
11/07/2024 22:44:33 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 4
11/07/2024 22:44:33 - INFO - __main__ -   Total optimization steps per epoch 21621
11/07/2024 22:44:33 - INFO - __main__ -   Total optimization steps = 216210
  0% 0/216210 [00:00<?, ?it/s]11/07/2024 22:44:33 - INFO - __main__ - Epoch 1/10
/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Traceback (most recent call last):
  File "/content/hw5_code/train.py", line 395, in <module>
    main()
  File "/content/hw5_code/train.py", line 297, in main
    for step, (images, labels) in enumerate(train_loader):
  File "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py", line 630, in __next__
    data = self._next_data()
  File "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py", line 1344, in _next_data
    return self._process_data(data)
  File "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py", line 1370, in _process_data
    data.reraise()
  File "/usr/local/lib/python3.10/dist-packages/torch/_utils.py", line 706, in reraise
    raise exception
RuntimeError: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/worker.py", line 309, in _worker_loop
    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]
  File "/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py", line 55, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py", line 317, in default_collate
    return collate(batch, collate_fn_map=default_collate_fn_map)
  File "/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py", line 174, in collate
    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.
  File "/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py", line 174, in <listcomp>
    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.
  File "/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py", line 142, in collate
    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)
  File "/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py", line 213, in collate_tensor_fn
    out = elem.new(storage).resize_(len(batch), *list(elem.size()))
RuntimeError: Trying to resize storage that is not resizable
